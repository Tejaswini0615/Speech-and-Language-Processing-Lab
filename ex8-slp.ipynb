{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exericse 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T15:58:45.208489Z",
     "iopub.status.busy": "2025-11-05T15:58:45.208178Z",
     "iopub.status.idle": "2025-11-05T15:58:45.675500Z",
     "shell.execute_reply": "2025-11-05T15:58:45.674791Z",
     "shell.execute_reply.started": "2025-11-05T15:58:45.208466Z"
    },
    "id": "hWs6rW0AwEOI",
    "outputId": "fade955a-d5c0-47d3-c436-84cb8224bd23",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-11-05 15:58:45--  http://www.manythings.org/anki/fra-eng.zip\n",
      "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
      "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8186368 (7.8M) [application/zip]\n",
      "Saving to: ‘fra-eng.zip.1’\n",
      "\n",
      "fra-eng.zip.1       100%[===================>]   7.81M  38.5MB/s    in 0.2s    \n",
      "\n",
      "2025-11-05 15:58:45 (38.5 MB/s) - ‘fra-eng.zip.1’ saved [8186368/8186368]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T15:58:58.508268Z",
     "iopub.status.busy": "2025-11-05T15:58:58.507958Z",
     "iopub.status.idle": "2025-11-05T15:58:58.905586Z",
     "shell.execute_reply": "2025-11-05T15:58:58.904782Z",
     "shell.execute_reply.started": "2025-11-05T15:58:58.508244Z"
    },
    "id": "_TmnCeNVwIIq",
    "outputId": "ea7c344d-c327-4185-f9a5-0dcc0a69b1b1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  fra-eng.zip\n",
      "  inflating: fra-eng-extracted/_about.txt  \n",
      "  inflating: fra-eng-extracted/fra.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip -o fra-eng.zip -d fra-eng-extracted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "68b305e7"
   },
   "source": [
    "### Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2025-11-05T16:01:08.188941Z",
     "iopub.status.busy": "2025-11-05T16:01:08.188645Z",
     "iopub.status.idle": "2025-11-05T16:01:20.493999Z",
     "shell.execute_reply": "2025-11-05T16:01:20.493016Z",
     "shell.execute_reply.started": "2025-11-05T16:01:08.188920Z"
    },
    "id": "7be4a255",
    "outputId": "6a9a552b-4b22-4f0d-bc3c-da618a794543",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  fra-eng.zip\n",
      "  inflating: fra-eng-extracted/_about.txt  \n",
      "  inflating: fra-eng-extracted/fra.txt  \n",
      "Extracted files: ['fra.txt', '_about.txt']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>french</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Marche.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>En route !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Bouge !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english      french\n",
       "0     Go.        Va !\n",
       "1     Go.     Marche.\n",
       "2     Go.  En route !\n",
       "3     Go.     Bouge !\n",
       "4     Hi.     Salut !"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete.\n",
      "English Vocab Size: 16377\n",
      "French Vocab Size: 22804\n",
      "Max Sequence Length: 20\n",
      "Training data shape: (191351, 20)\n",
      "Validation data shape: (47838, 20)\n"
     ]
    }
   ],
   "source": [
    "!unzip -o fra-eng.zip -d fra-eng-extracted\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Check extracted files\n",
    "print(\"Extracted files:\", os.listdir('/kaggle/working/fra-eng-extracted'))\n",
    "\n",
    "# Load the dataset (correct path for Kaggle)\n",
    "file_path = '/kaggle/working/fra-eng-extracted/fra.txt'\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{file_path}' not found. Please check extraction output.\")\n",
    "    lines = []  # prevent crash\n",
    "\n",
    "# Create sentence pairs\n",
    "sentence_pairs = []\n",
    "for line in lines:\n",
    "    if line:\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) >= 2:\n",
    "            sentence_pairs.append((parts[0], parts[1]))\n",
    "\n",
    "# Create DataFrame\n",
    "if sentence_pairs:\n",
    "    df = pd.DataFrame(sentence_pairs, columns=['english', 'french'])\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No data loaded. Please check the file path and content.\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    return sentence.strip()\n",
    "\n",
    "# Apply preprocessing\n",
    "if not df.empty:\n",
    "    df['english_processed'] = df['english'].apply(preprocess_sentence)\n",
    "    df['french_processed'] = df['french'].apply(preprocess_sentence).apply(lambda x: '<start> ' + x + ' <end>')\n",
    "\n",
    "    # Tokenization\n",
    "    english_tokenizer = Tokenizer(filters='')\n",
    "    english_tokenizer.fit_on_texts(df['english_processed'])\n",
    "    english_sequences = english_tokenizer.texts_to_sequences(df['english_processed'])\n",
    "    english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "\n",
    "    french_tokenizer = Tokenizer(filters='')\n",
    "    french_tokenizer.fit_on_texts(df['french_processed'])\n",
    "    french_sequences = french_tokenizer.texts_to_sequences(df['french_processed'])\n",
    "    french_vocab_size = len(french_tokenizer.word_index) + 1\n",
    "\n",
    "    max_sequence_length = 20\n",
    "    english_padded = pad_sequences(english_sequences, maxlen=max_sequence_length, padding='post')\n",
    "    french_padded = pad_sequences(french_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        english_padded, french_padded, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(\"✅ Preprocessing complete.\")\n",
    "    print(f\"English Vocab Size: {english_vocab_size}\")\n",
    "    print(f\"French Vocab Size: {french_vocab_size}\")\n",
    "    print(f\"Max Sequence Length: {max_sequence_length}\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Validation data shape: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:04:51.383714Z",
     "iopub.status.busy": "2025-11-05T16:04:51.382927Z",
     "iopub.status.idle": "2025-11-05T16:57:07.083005Z",
     "shell.execute_reply": "2025-11-05T16:57:07.082192Z",
     "shell.execute_reply.started": "2025-11-05T16:04:51.383689Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training: 10 epochs, 2989 steps per epoch, batch size 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762358699.088817     115 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 10.0347\n",
      "Epoch 1 Batch 100 Loss 3.9308\n",
      "Epoch 1 Batch 200 Loss 4.6019\n",
      "Epoch 1 Batch 300 Loss 4.0095\n",
      "Epoch 1 Batch 400 Loss 2.7727\n",
      "Epoch 1 Batch 500 Loss 4.0504\n",
      "Epoch 1 Batch 600 Loss 4.1818\n",
      "Epoch 1 Batch 700 Loss 3.5200\n",
      "Epoch 1 Batch 800 Loss 3.1758\n",
      "Epoch 1 Batch 900 Loss 3.5229\n",
      "Epoch 1 Batch 1000 Loss 3.6028\n",
      "Epoch 1 Batch 1100 Loss 3.5112\n",
      "Epoch 1 Batch 1200 Loss 3.7352\n",
      "Epoch 1 Batch 1300 Loss 3.6991\n",
      "Epoch 1 Batch 1400 Loss 2.9896\n",
      "Epoch 1 Batch 1500 Loss 3.4178\n",
      "Epoch 1 Batch 1600 Loss 3.3253\n",
      "Epoch 1 Batch 1700 Loss 3.0043\n",
      "Epoch 1 Batch 1800 Loss 3.2857\n",
      "Epoch 1 Batch 1900 Loss 2.8163\n",
      "Epoch 1 Batch 2000 Loss 3.0724\n",
      "Epoch 1 Batch 2100 Loss 3.0622\n",
      "Epoch 1 Batch 2200 Loss 3.3256\n",
      "Epoch 1 Batch 2300 Loss 2.3969\n",
      "Epoch 1 Batch 2400 Loss 2.7687\n",
      "Epoch 1 Batch 2500 Loss 2.5497\n",
      "Epoch 1 Batch 2600 Loss 2.4176\n",
      "Epoch 1 Batch 2700 Loss 2.2281\n",
      "Epoch 1 Batch 2800 Loss 2.3846\n",
      "Epoch 1 Batch 2900 Loss 2.8446\n",
      "Epoch 1 Loss 3.3436\n",
      "Time taken for 1 epoch 317.31 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 2.6650\n",
      "Epoch 2 Batch 100 Loss 2.0547\n",
      "Epoch 2 Batch 200 Loss 2.0614\n",
      "Epoch 2 Batch 300 Loss 1.9965\n",
      "Epoch 2 Batch 400 Loss 3.5420\n",
      "Epoch 2 Batch 500 Loss 2.5644\n",
      "Epoch 2 Batch 600 Loss 2.8969\n",
      "Epoch 2 Batch 700 Loss 2.0388\n",
      "Epoch 2 Batch 800 Loss 2.1177\n",
      "Epoch 2 Batch 900 Loss 2.6814\n",
      "Epoch 2 Batch 1000 Loss 2.3941\n",
      "Epoch 2 Batch 1100 Loss 2.0737\n",
      "Epoch 2 Batch 1200 Loss 2.2179\n",
      "Epoch 2 Batch 1300 Loss 2.3616\n",
      "Epoch 2 Batch 1400 Loss 2.3972\n",
      "Epoch 2 Batch 1500 Loss 2.2631\n",
      "Epoch 2 Batch 1600 Loss 2.0824\n",
      "Epoch 2 Batch 1700 Loss 2.0717\n",
      "Epoch 2 Batch 1800 Loss 2.2888\n",
      "Epoch 2 Batch 1900 Loss 2.3775\n",
      "Epoch 2 Batch 2000 Loss 1.8129\n",
      "Epoch 2 Batch 2100 Loss 2.1415\n",
      "Epoch 2 Batch 2200 Loss 1.8917\n",
      "Epoch 2 Batch 2300 Loss 2.3030\n",
      "Epoch 2 Batch 2400 Loss 1.3970\n",
      "Epoch 2 Batch 2500 Loss 2.3271\n",
      "Epoch 2 Batch 2600 Loss 1.8795\n",
      "Epoch 2 Batch 2700 Loss 2.3979\n",
      "Epoch 2 Batch 2800 Loss 1.6051\n",
      "Epoch 2 Batch 2900 Loss 1.8804\n",
      "Epoch 2 Loss 2.2129\n",
      "Time taken for 1 epoch 312.85 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.7389\n",
      "Epoch 3 Batch 100 Loss 2.2938\n",
      "Epoch 3 Batch 200 Loss 1.7907\n",
      "Epoch 3 Batch 300 Loss 1.4139\n",
      "Epoch 3 Batch 400 Loss 1.5856\n",
      "Epoch 3 Batch 500 Loss 1.6093\n",
      "Epoch 3 Batch 600 Loss 1.9332\n",
      "Epoch 3 Batch 700 Loss 1.8812\n",
      "Epoch 3 Batch 800 Loss 1.6073\n",
      "Epoch 3 Batch 900 Loss 1.6974\n",
      "Epoch 3 Batch 1000 Loss 1.9561\n",
      "Epoch 3 Batch 1100 Loss 1.7132\n",
      "Epoch 3 Batch 1200 Loss 1.4676\n",
      "Epoch 3 Batch 1300 Loss 1.5855\n",
      "Epoch 3 Batch 1400 Loss 1.5676\n",
      "Epoch 3 Batch 1500 Loss 1.3158\n",
      "Epoch 3 Batch 1600 Loss 2.0262\n",
      "Epoch 3 Batch 1700 Loss 1.7981\n",
      "Epoch 3 Batch 1800 Loss 1.5074\n",
      "Epoch 3 Batch 1900 Loss 2.1323\n",
      "Epoch 3 Batch 2000 Loss 1.3945\n",
      "Epoch 3 Batch 2100 Loss 1.9680\n",
      "Epoch 3 Batch 2200 Loss 1.9341\n",
      "Epoch 3 Batch 2300 Loss 1.5628\n",
      "Epoch 3 Batch 2400 Loss 1.7725\n",
      "Epoch 3 Batch 2500 Loss 1.3949\n",
      "Epoch 3 Batch 2600 Loss 1.3155\n",
      "Epoch 3 Batch 2700 Loss 1.5401\n",
      "Epoch 3 Batch 2800 Loss 1.2911\n",
      "Epoch 3 Batch 2900 Loss 1.0590\n",
      "Epoch 3 Loss 1.6806\n",
      "Time taken for 1 epoch 312.51 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.1141\n",
      "Epoch 4 Batch 100 Loss 1.3361\n",
      "Epoch 4 Batch 200 Loss 1.3036\n",
      "Epoch 4 Batch 300 Loss 1.1137\n",
      "Epoch 4 Batch 400 Loss 1.6706\n",
      "Epoch 4 Batch 500 Loss 1.1261\n",
      "Epoch 4 Batch 600 Loss 1.2766\n",
      "Epoch 4 Batch 700 Loss 1.5559\n",
      "Epoch 4 Batch 800 Loss 1.0434\n",
      "Epoch 4 Batch 900 Loss 1.7346\n",
      "Epoch 4 Batch 1000 Loss 1.5911\n",
      "Epoch 4 Batch 1100 Loss 1.3274\n",
      "Epoch 4 Batch 1200 Loss 1.4276\n",
      "Epoch 4 Batch 1300 Loss 0.9851\n",
      "Epoch 4 Batch 1400 Loss 1.3414\n",
      "Epoch 4 Batch 1500 Loss 1.3123\n",
      "Epoch 4 Batch 1600 Loss 1.3635\n",
      "Epoch 4 Batch 1700 Loss 1.6849\n",
      "Epoch 4 Batch 1800 Loss 1.2471\n",
      "Epoch 4 Batch 1900 Loss 1.2054\n",
      "Epoch 4 Batch 2000 Loss 1.1820\n",
      "Epoch 4 Batch 2100 Loss 1.2245\n",
      "Epoch 4 Batch 2200 Loss 1.4184\n",
      "Epoch 4 Batch 2300 Loss 1.3937\n",
      "Epoch 4 Batch 2400 Loss 1.3441\n",
      "Epoch 4 Batch 2500 Loss 1.3302\n",
      "Epoch 4 Batch 2600 Loss 1.2386\n",
      "Epoch 4 Batch 2700 Loss 1.0688\n",
      "Epoch 4 Batch 2800 Loss 1.4723\n",
      "Epoch 4 Batch 2900 Loss 1.1408\n",
      "Epoch 4 Loss 1.2812\n",
      "Time taken for 1 epoch 314.14 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.1374\n",
      "Epoch 5 Batch 100 Loss 1.2794\n",
      "Epoch 5 Batch 200 Loss 1.2112\n",
      "Epoch 5 Batch 300 Loss 1.1154\n",
      "Epoch 5 Batch 400 Loss 0.9500\n",
      "Epoch 5 Batch 500 Loss 0.9355\n",
      "Epoch 5 Batch 600 Loss 1.1592\n",
      "Epoch 5 Batch 700 Loss 0.9901\n",
      "Epoch 5 Batch 800 Loss 1.1665\n",
      "Epoch 5 Batch 900 Loss 0.9872\n",
      "Epoch 5 Batch 1000 Loss 0.9184\n",
      "Epoch 5 Batch 1100 Loss 0.8820\n",
      "Epoch 5 Batch 1200 Loss 1.0910\n",
      "Epoch 5 Batch 1300 Loss 0.9163\n",
      "Epoch 5 Batch 1400 Loss 0.9752\n",
      "Epoch 5 Batch 1500 Loss 0.9319\n",
      "Epoch 5 Batch 1600 Loss 1.1415\n",
      "Epoch 5 Batch 1700 Loss 1.0073\n",
      "Epoch 5 Batch 1800 Loss 1.0838\n",
      "Epoch 5 Batch 1900 Loss 1.0006\n",
      "Epoch 5 Batch 2000 Loss 1.0879\n",
      "Epoch 5 Batch 2100 Loss 1.0077\n",
      "Epoch 5 Batch 2200 Loss 0.8898\n",
      "Epoch 5 Batch 2300 Loss 1.0843\n",
      "Epoch 5 Batch 2400 Loss 1.1047\n",
      "Epoch 5 Batch 2500 Loss 1.2725\n",
      "Epoch 5 Batch 2600 Loss 0.8701\n",
      "Epoch 5 Batch 2700 Loss 1.0766\n",
      "Epoch 5 Batch 2800 Loss 0.9046\n",
      "Epoch 5 Batch 2900 Loss 0.8043\n",
      "Epoch 5 Loss 0.9972\n",
      "Time taken for 1 epoch 313.34 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.9942\n",
      "Epoch 6 Batch 100 Loss 0.8407\n",
      "Epoch 6 Batch 200 Loss 1.0574\n",
      "Epoch 6 Batch 300 Loss 0.6369\n",
      "Epoch 6 Batch 400 Loss 0.7349\n",
      "Epoch 6 Batch 500 Loss 0.8381\n",
      "Epoch 6 Batch 600 Loss 0.7775\n",
      "Epoch 6 Batch 700 Loss 0.6597\n",
      "Epoch 6 Batch 800 Loss 0.9230\n",
      "Epoch 6 Batch 900 Loss 0.8174\n",
      "Epoch 6 Batch 1000 Loss 0.7127\n",
      "Epoch 6 Batch 1100 Loss 0.6950\n",
      "Epoch 6 Batch 1200 Loss 0.8772\n",
      "Epoch 6 Batch 1300 Loss 0.7777\n",
      "Epoch 6 Batch 1400 Loss 0.8610\n",
      "Epoch 6 Batch 1500 Loss 0.6855\n",
      "Epoch 6 Batch 1600 Loss 0.8043\n",
      "Epoch 6 Batch 1700 Loss 0.6871\n",
      "Epoch 6 Batch 1800 Loss 0.6974\n",
      "Epoch 6 Batch 1900 Loss 0.6574\n",
      "Epoch 6 Batch 2000 Loss 0.5668\n",
      "Epoch 6 Batch 2100 Loss 0.9244\n",
      "Epoch 6 Batch 2200 Loss 0.9011\n",
      "Epoch 6 Batch 2300 Loss 0.8201\n",
      "Epoch 6 Batch 2400 Loss 0.9610\n",
      "Epoch 6 Batch 2500 Loss 0.6926\n",
      "Epoch 6 Batch 2600 Loss 0.8813\n",
      "Epoch 6 Batch 2700 Loss 0.7201\n",
      "Epoch 6 Batch 2800 Loss 0.9085\n",
      "Epoch 6 Batch 2900 Loss 0.7287\n",
      "Epoch 6 Loss 0.7944\n",
      "Time taken for 1 epoch 312.42 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.6845\n",
      "Epoch 7 Batch 100 Loss 0.6449\n",
      "Epoch 7 Batch 200 Loss 0.5889\n",
      "Epoch 7 Batch 300 Loss 0.5727\n",
      "Epoch 7 Batch 400 Loss 0.6602\n",
      "Epoch 7 Batch 500 Loss 0.5835\n",
      "Epoch 7 Batch 600 Loss 0.6393\n",
      "Epoch 7 Batch 700 Loss 0.6153\n",
      "Epoch 7 Batch 800 Loss 0.5578\n",
      "Epoch 7 Batch 900 Loss 0.6356\n",
      "Epoch 7 Batch 1000 Loss 0.7130\n",
      "Epoch 7 Batch 1100 Loss 0.6999\n",
      "Epoch 7 Batch 1200 Loss 0.5827\n",
      "Epoch 7 Batch 1300 Loss 0.5156\n",
      "Epoch 7 Batch 1400 Loss 0.6487\n",
      "Epoch 7 Batch 1500 Loss 0.6168\n",
      "Epoch 7 Batch 1600 Loss 0.8632\n",
      "Epoch 7 Batch 1700 Loss 0.4561\n",
      "Epoch 7 Batch 1800 Loss 0.6120\n",
      "Epoch 7 Batch 1900 Loss 0.5222\n",
      "Epoch 7 Batch 2000 Loss 0.7377\n",
      "Epoch 7 Batch 2100 Loss 0.5546\n",
      "Epoch 7 Batch 2200 Loss 0.6208\n",
      "Epoch 7 Batch 2300 Loss 0.7072\n",
      "Epoch 7 Batch 2400 Loss 0.5496\n",
      "Epoch 7 Batch 2500 Loss 0.5090\n",
      "Epoch 7 Batch 2600 Loss 0.6851\n",
      "Epoch 7 Batch 2700 Loss 0.6418\n",
      "Epoch 7 Batch 2800 Loss 0.7019\n",
      "Epoch 7 Batch 2900 Loss 0.7359\n",
      "Epoch 7 Loss 0.6492\n",
      "Time taken for 1 epoch 312.37 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.6853\n",
      "Epoch 8 Batch 100 Loss 0.6589\n",
      "Epoch 8 Batch 200 Loss 0.4206\n",
      "Epoch 8 Batch 300 Loss 0.5129\n",
      "Epoch 8 Batch 400 Loss 0.5136\n",
      "Epoch 8 Batch 500 Loss 0.6518\n",
      "Epoch 8 Batch 600 Loss 0.5916\n",
      "Epoch 8 Batch 700 Loss 0.4867\n",
      "Epoch 8 Batch 800 Loss 0.6043\n",
      "Epoch 8 Batch 900 Loss 0.5735\n",
      "Epoch 8 Batch 1000 Loss 0.6417\n",
      "Epoch 8 Batch 1100 Loss 0.5647\n",
      "Epoch 8 Batch 1200 Loss 0.4648\n",
      "Epoch 8 Batch 1300 Loss 0.4933\n",
      "Epoch 8 Batch 1400 Loss 0.4226\n",
      "Epoch 8 Batch 1500 Loss 0.5701\n",
      "Epoch 8 Batch 1600 Loss 0.5899\n",
      "Epoch 8 Batch 1700 Loss 0.5790\n",
      "Epoch 8 Batch 1800 Loss 0.5759\n",
      "Epoch 8 Batch 1900 Loss 0.6172\n",
      "Epoch 8 Batch 2000 Loss 0.5959\n",
      "Epoch 8 Batch 2100 Loss 0.5392\n",
      "Epoch 8 Batch 2200 Loss 0.5658\n",
      "Epoch 8 Batch 2300 Loss 0.5945\n",
      "Epoch 8 Batch 2400 Loss 0.6507\n",
      "Epoch 8 Batch 2500 Loss 0.5540\n",
      "Epoch 8 Batch 2600 Loss 0.5099\n",
      "Epoch 8 Batch 2700 Loss 0.4474\n",
      "Epoch 8 Batch 2800 Loss 0.7047\n",
      "Epoch 8 Batch 2900 Loss 0.6621\n",
      "Epoch 8 Loss 0.5427\n",
      "Time taken for 1 epoch 313.96 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.4419\n",
      "Epoch 9 Batch 100 Loss 0.3731\n",
      "Epoch 9 Batch 200 Loss 0.3984\n",
      "Epoch 9 Batch 300 Loss 0.3886\n",
      "Epoch 9 Batch 400 Loss 0.3668\n",
      "Epoch 9 Batch 500 Loss 0.3548\n",
      "Epoch 9 Batch 600 Loss 0.4754\n",
      "Epoch 9 Batch 700 Loss 0.4612\n",
      "Epoch 9 Batch 800 Loss 0.4797\n",
      "Epoch 9 Batch 900 Loss 0.5233\n",
      "Epoch 9 Batch 1000 Loss 0.4812\n",
      "Epoch 9 Batch 1100 Loss 0.4548\n",
      "Epoch 9 Batch 1200 Loss 0.3801\n",
      "Epoch 9 Batch 1300 Loss 0.3439\n",
      "Epoch 9 Batch 1400 Loss 0.4218\n",
      "Epoch 9 Batch 1500 Loss 0.5040\n",
      "Epoch 9 Batch 1600 Loss 0.4023\n",
      "Epoch 9 Batch 1700 Loss 0.5443\n",
      "Epoch 9 Batch 1800 Loss 0.5673\n",
      "Epoch 9 Batch 1900 Loss 0.4285\n",
      "Epoch 9 Batch 2000 Loss 0.3559\n",
      "Epoch 9 Batch 2100 Loss 0.6045\n",
      "Epoch 9 Batch 2200 Loss 0.5306\n",
      "Epoch 9 Batch 2300 Loss 0.4471\n",
      "Epoch 9 Batch 2400 Loss 0.4878\n",
      "Epoch 9 Batch 2500 Loss 0.4714\n",
      "Epoch 9 Batch 2600 Loss 0.4133\n",
      "Epoch 9 Batch 2700 Loss 0.5550\n",
      "Epoch 9 Batch 2800 Loss 0.5796\n",
      "Epoch 9 Batch 2900 Loss 0.5912\n",
      "Epoch 9 Loss 0.4618\n",
      "Time taken for 1 epoch 312.37 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.4537\n",
      "Epoch 10 Batch 100 Loss 0.3377\n",
      "Epoch 10 Batch 200 Loss 0.3794\n",
      "Epoch 10 Batch 300 Loss 0.3567\n",
      "Epoch 10 Batch 400 Loss 0.3619\n",
      "Epoch 10 Batch 500 Loss 0.3314\n",
      "Epoch 10 Batch 600 Loss 0.5086\n",
      "Epoch 10 Batch 700 Loss 0.3940\n",
      "Epoch 10 Batch 800 Loss 0.4071\n",
      "Epoch 10 Batch 900 Loss 0.4226\n",
      "Epoch 10 Batch 1000 Loss 0.3173\n",
      "Epoch 10 Batch 1100 Loss 0.3803\n",
      "Epoch 10 Batch 1200 Loss 0.3223\n",
      "Epoch 10 Batch 1300 Loss 0.4133\n",
      "Epoch 10 Batch 1400 Loss 0.3407\n",
      "Epoch 10 Batch 1500 Loss 0.4314\n",
      "Epoch 10 Batch 1600 Loss 0.4753\n",
      "Epoch 10 Batch 1700 Loss 0.4003\n",
      "Epoch 10 Batch 1800 Loss 0.4556\n",
      "Epoch 10 Batch 1900 Loss 0.3294\n",
      "Epoch 10 Batch 2000 Loss 0.3729\n",
      "Epoch 10 Batch 2100 Loss 0.3190\n",
      "Epoch 10 Batch 2200 Loss 0.4061\n",
      "Epoch 10 Batch 2300 Loss 0.3765\n",
      "Epoch 10 Batch 2400 Loss 0.3692\n",
      "Epoch 10 Batch 2500 Loss 0.3133\n",
      "Epoch 10 Batch 2600 Loss 0.3685\n",
      "Epoch 10 Batch 2700 Loss 0.4743\n",
      "Epoch 10 Batch 2800 Loss 0.4198\n",
      "Epoch 10 Batch 2900 Loss 0.3411\n",
      "Epoch 10 Loss 0.3977\n",
      "Time taken for 1 epoch 312.75 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Ensure these exist from previous cells:\n",
    "# encoder, decoder, english_word_to_index, french_word_to_index,\n",
    "# max_sequence_length, lstm_units, X_train, y_train\n",
    "\n",
    "optimizer = Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    # real shape: (batch_size,)\n",
    "    # pred shape: (batch_size, vocab_size)\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    # return average over non-masked tokens\n",
    "    denom = tf.reduce_sum(mask) + 1e-7\n",
    "    return tf.reduce_sum(loss_) / denom\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ):\n",
    "    # inp: (batch_size, seq_len), targ: (batch_size, seq_len)\n",
    "    batch_size = tf.shape(inp)[0]\n",
    "    enc_hidden = encoder.initialize_hidden_state(batch_size)\n",
    "    loss = tf.constant(0.0)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp, enc_hidden)\n",
    "        dec_hidden = [enc_hidden_h, enc_hidden_c]\n",
    "\n",
    "        # prepare decoder input: batch of <start> tokens\n",
    "        start_token_id = tf.cast(french_word_to_index['<start>'], tf.int32)\n",
    "        dec_input = tf.expand_dims(tf.fill([batch_size], start_token_id), 1)  # shape (batch, 1)\n",
    "\n",
    "        # teacher forcing loop\n",
    "        seq_len = tf.shape(targ)[1]\n",
    "        for t in tf.range(1, seq_len):\n",
    "            # predictions shape -> (batch, vocab_size) after decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            # predictions has been reshaped inside decoder to (batch, vocab_size)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    # compute gradients and apply (handle None grads)\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    grads_and_vars = []\n",
    "    for g, v in zip(gradients, variables):\n",
    "        if g is None:\n",
    "            # skip None gradients\n",
    "            continue\n",
    "        grads_and_vars.append((g, v))\n",
    "    if grads_and_vars:\n",
    "        optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "    # return average loss per time-step (scalar)\n",
    "    seq_len_f = tf.cast(seq_len - 1, tf.float32)  # we started from t=1\n",
    "    batch_loss = loss / seq_len_f\n",
    "    return batch_loss\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Build tf.data Dataset (ensure X_train/y_train are numpy arrays)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=len(X_train), seed=42).batch(BATCH_SIZE, drop_remainder=True)\n",
    "steps_per_epoch = tf.data.experimental.cardinality(dataset).numpy()\n",
    "\n",
    "print(f\"Starting training: {EPOCHS} epochs, {steps_per_epoch} steps per epoch, batch size {BATCH_SIZE}\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0.0\n",
    "    for batch, (inp, targ) in enumerate(dataset):\n",
    "        batch_loss = train_step(inp, targ)\n",
    "        total_loss += batch_loss.numpy()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}\")\n",
    "\n",
    "    epoch_loss = total_loss / max(1, steps_per_epoch)\n",
    "    print(f'Epoch {epoch+1} Loss {epoch_loss:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION (Greedy + Beam) and BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T16:58:02.877115Z",
     "iopub.status.busy": "2025-11-05T16:58:02.876681Z",
     "iopub.status.idle": "2025-11-05T16:59:15.279376Z",
     "shell.execute_reply": "2025-11-05T16:59:15.278666Z",
     "shell.execute_reply.started": "2025-11-05T16:58:02.877091Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 200 random validation samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/200 - elapsed 18.3s\n",
      "Processed 100/200 - elapsed 36.8s\n",
      "Processed 150/200 - elapsed 54.9s\n",
      "Processed 200/200 - elapsed 71.8s\n",
      "\n",
      "=== Evaluation Results ===\n",
      "Greedy Corpus BLEU (subset of 200): 0.5725\n",
      "Beam (k=3) Corpus BLEU (subset of 200): 0.5607\n",
      "Average per-sentence BLEU (greedy): 0.4460\n",
      "Average per-sentence BLEU (beam):   0.4301\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: EVALUATION (Greedy + Beam) and BLEU scores\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "# ---------- Helper: preprocess & greedy translate ----------\n",
    "def translate_sentence_greedy(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [english_word_to_index[w] for w in sentence.split() if w in english_word_to_index]\n",
    "    if len(inputs) == 0:\n",
    "        return \"\"  # nothing to translate\n",
    "    inputs = pad_sequences([inputs], maxlen=max_sequence_length, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    hidden = [tf.zeros((1, lstm_units)), tf.zeros((1, lstm_units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, hidden)\n",
    "    dec_hidden = [enc_h, enc_c]\n",
    "    dec_input = tf.expand_dims([french_word_to_index['<start>']], 0)\n",
    "\n",
    "    result_tokens = []\n",
    "    for t in range(max_sequence_length):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy().astype(int)\n",
    "        predicted_word = french_index_to_word.get(predicted_id, '<unk>')\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "        result_tokens.append(predicted_word)\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return ' '.join(result_tokens).strip()\n",
    "\n",
    "# ---------- Helper: beam search translate ----------\n",
    "def translate_sentence_beam_search(sentence, beam_width=3):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [english_word_to_index[w] for w in sentence.split() if w in english_word_to_index]\n",
    "    if len(inputs) == 0:\n",
    "        return \"\"\n",
    "    inputs = pad_sequences([inputs], maxlen=max_sequence_length, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    hidden = [tf.zeros((1, lstm_units)), tf.zeros((1, lstm_units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, hidden)\n",
    "    enc_hidden = [enc_h, enc_c]\n",
    "\n",
    "    start_token = french_word_to_index['<start>']\n",
    "    candidates = [(0.0, [start_token], enc_hidden)]  # score, token_seq, hidden_state\n",
    "\n",
    "    for _ in range(max_sequence_length):\n",
    "        all_candidates = []\n",
    "        for score, seq, hid in candidates:\n",
    "            last_token = seq[-1]\n",
    "            if last_token == french_word_to_index['<end>']:\n",
    "                all_candidates.append((score, seq, hid))\n",
    "                continue\n",
    "            dec_input = tf.expand_dims([last_token], 0)\n",
    "            predictions, new_hidden, _ = decoder(dec_input, hid, enc_out)  # predictions (1, vocab)\n",
    "            # take log-probs for stability\n",
    "            log_probs = tf.math.log(tf.nn.softmax(predictions[0]) + 1e-9).numpy()\n",
    "            top_k_ids = np.argsort(log_probs)[-beam_width:][::-1]  # descending\n",
    "            top_k_scores = log_probs[top_k_ids]\n",
    "            for i_id, i_score in zip(top_k_ids, top_k_scores):\n",
    "                new_score = score + float(i_score)\n",
    "                new_seq = seq + [int(i_id)]\n",
    "                all_candidates.append((new_score, new_seq, new_hidden))\n",
    "        # keep best beam_width\n",
    "        candidates = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_width]\n",
    "\n",
    "    best_seq = candidates[0][1]\n",
    "    tokens = []\n",
    "    for token_id in best_seq:\n",
    "        if token_id == french_word_to_index['<start>'] or token_id == french_word_to_index['<end>']:\n",
    "            continue\n",
    "        tokens.append(french_index_to_word.get(token_id, '<unk>'))\n",
    "    return ' '.join(tokens).strip()\n",
    "\n",
    "# ---------- BLEU helpers ----------\n",
    "def calculate_sentence_bleu(reference_sentence, translated_sentence):\n",
    "    ref_tokens = [reference_sentence.split()]\n",
    "    trans_tokens = translated_sentence.split()\n",
    "    if not ref_tokens or not ref_tokens[0] or not trans_tokens:\n",
    "        return 0.0\n",
    "    return sentence_bleu(ref_tokens, trans_tokens)\n",
    "\n",
    "def calculate_corpus_bleu(reference_list, translated_list):\n",
    "    list_of_references = [[ref.split()] for ref in reference_list]\n",
    "    list_of_translations = [t.split() for t in translated_list]\n",
    "    if len(list_of_references) == 0:\n",
    "        return 0.0\n",
    "    return corpus_bleu(list_of_references, list_of_translations)\n",
    "\n",
    "# ---------- Evaluate on a random subset of validation set ----------\n",
    "num_eval_samples = 200\n",
    "total_val = len(X_val)\n",
    "if num_eval_samples > total_val:\n",
    "    num_eval_samples = total_val\n",
    "\n",
    "random_indices = random.sample(range(total_val), num_eval_samples)\n",
    "print(f\"Evaluating {num_eval_samples} random validation samples...\")\n",
    "\n",
    "start_t = time.time()\n",
    "refs = []\n",
    "greedy_trans = []\n",
    "beam_trans = []\n",
    "greedy_scores = []\n",
    "beam_scores = []\n",
    "\n",
    "for idx_count, idx in enumerate(random_indices):\n",
    "    # Get processed English and French from df directly using original indices.\n",
    "    # NOTE: we assume the df ordering corresponds to english_padded_sequences indices.\n",
    "    # If you used different splitting logic, store indices during split to map them precisely.\n",
    "    eng_proc = df['english_processed'].iloc[idx]\n",
    "    fr_proc = df['french_processed'].iloc[idx].replace('<start> ', '').replace(' <end>', '')\n",
    "\n",
    "    # Greedy translation\n",
    "    g_t = translate_sentence_greedy(eng_proc)\n",
    "    greedy_trans.append(g_t)\n",
    "    refs.append(fr_proc)\n",
    "    greedy_scores.append(calculate_sentence_bleu(fr_proc, g_t))\n",
    "\n",
    "    # Beam translation\n",
    "    b_t = translate_sentence_beam_search(eng_proc, beam_width=3)\n",
    "    beam_trans.append(b_t)\n",
    "    beam_scores.append(calculate_sentence_bleu(fr_proc, b_t))\n",
    "\n",
    "    # Progress\n",
    "    if (idx_count + 1) % 50 == 0:\n",
    "        elapsed = time.time() - start_t\n",
    "        print(f\"Processed {idx_count+1}/{num_eval_samples} - elapsed {elapsed:.1f}s\")\n",
    "\n",
    "# Corpus BLEU\n",
    "greedy_corpus_bleu = calculate_corpus_bleu(refs, greedy_trans)\n",
    "beam_corpus_bleu = calculate_corpus_bleu(refs, beam_trans)\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(f\"Greedy Corpus BLEU (subset of {num_eval_samples}): {greedy_corpus_bleu:.4f}\")\n",
    "print(f\"Beam (k=3) Corpus BLEU (subset of {num_eval_samples}): {beam_corpus_bleu:.4f}\")\n",
    "print(f\"Average per-sentence BLEU (greedy): {np.mean(greedy_scores):.4f}\")\n",
    "print(f\"Average per-sentence BLEU (beam):   {np.mean(beam_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
