{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exericse 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-05T14:18:04.304104Z",
     "iopub.status.busy": "2025-11-05T14:18:04.303827Z",
     "iopub.status.idle": "2025-11-05T14:18:04.584380Z",
     "shell.execute_reply": "2025-11-05T14:18:04.583375Z",
     "shell.execute_reply.started": "2025-11-05T14:18:04.304084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'chitchat-dataset' already exists and is not an empty directory.\n",
      "chitchat_dataset  examples  Makefile\t pyproject.toml\n",
      "CONTRIBUTING.md   LICENSE   poetry.lock  README.md\n"
     ]
    }
   ],
   "source": [
    "# Clone the chitchat dataset from GitHub\n",
    "!git clone https://github.com/BYU-PCCL/chitchat-dataset.git\n",
    "\n",
    "# Navigate into the folder and list files\n",
    "!ls chitchat-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T14:19:50.960622Z",
     "iopub.status.busy": "2025-11-05T14:19:50.960288Z",
     "iopub.status.idle": "2025-11-05T14:19:51.726118Z",
     "shell.execute_reply": "2025-11-05T14:19:51.725225Z",
     "shell.execute_reply.started": "2025-11-05T14:19:50.960599Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded dataset with 7168 entries.\n",
      "Top-level type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "dataset_path = \"/kaggle/working/chitchat-dataset/chitchat_dataset/dataset.json\"\n",
    "\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(\"âœ… Loaded dataset with\", len(dataset), \"entries.\")\n",
    "print(\"Top-level type:\", type(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T14:20:08.342683Z",
     "iopub.status.busy": "2025-11-05T14:20:08.341876Z",
     "iopub.status.idle": "2025-11-05T14:20:08.348068Z",
     "shell.execute_reply": "2025-11-05T14:20:08.347212Z",
     "shell.execute_reply.started": "2025-11-05T14:20:08.342652Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted 7168 conversation threads.\n",
      "Sample keys in first conversation: ['messages', 'ratings', 'start', 'prompt']\n"
     ]
    }
   ],
   "source": [
    "# Extract all conversation objects from dictionary values\n",
    "conversations = list(dataset.values())\n",
    "print(\"âœ… Extracted\", len(conversations), \"conversation threads.\")\n",
    "print(\"Sample keys in first conversation:\", list(conversations[0].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T14:20:10.408698Z",
     "iopub.status.busy": "2025-11-05T14:20:10.408374Z",
     "iopub.status.idle": "2025-11-05T14:20:10.877804Z",
     "shell.execute_reply": "2025-11-05T14:20:10.877097Z",
     "shell.execute_reply.started": "2025-11-05T14:20:10.408675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted 250635 (context, response) pairs from 7168 conversations.\n",
      "Sample pairs:\n",
      "1. Q: hello â†’ A: how are you doing today?\n",
      "2. Q: how are you doing today? â†’ A: whats up md\n",
      "3. Q: whats up md â†’ A: im doing good\n",
      "4. Q: im doing good â†’ A: how are you doing?\n",
      "5. Q: how are you doing? â†’ A: im alright, i just took a nap. but it was one of those naps that doesnt help anything. it just makes everything worse and you question all your life choices\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s,.!?']\", \"\", text)\n",
    "    return text\n",
    "\n",
    "pairs = []\n",
    "\n",
    "for conv in conversations:\n",
    "    if \"messages\" not in conv:\n",
    "        continue\n",
    "    messages = conv[\"messages\"]\n",
    "\n",
    "    # Flatten messages across sublists\n",
    "    all_msgs = []\n",
    "    for group in messages:\n",
    "        for msg in group:\n",
    "            if \"text\" in msg:\n",
    "                all_msgs.append(clean_text(msg[\"text\"]))\n",
    "\n",
    "    # Now create consecutive pairs (previous msg -> next msg)\n",
    "    for i in range(len(all_msgs) - 1):\n",
    "        context = all_msgs[i]\n",
    "        response = all_msgs[i + 1]\n",
    "        if context and response:\n",
    "            pairs.append((context, response))\n",
    "\n",
    "print(f\"âœ… Extracted {len(pairs)} (context, response) pairs from {len(conversations)} conversations.\")\n",
    "print(\"Sample pairs:\")\n",
    "for i in range(5):\n",
    "    print(f\"{i+1}. Q: {pairs[i][0]} â†’ A: {pairs[i][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T14:20:18.419254Z",
     "iopub.status.busy": "2025-11-05T14:20:18.418955Z",
     "iopub.status.idle": "2025-11-05T14:20:36.568900Z",
     "shell.execute_reply": "2025-11-05T14:20:36.568119Z",
     "shell.execute_reply.started": "2025-11-05T14:20:18.419231Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 14:20:20.077667: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762352420.279295     152 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762352420.340773     152 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenization done: 225571 train samples, 25064 test samples.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "MAX_LEN = 20\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "input_texts = [p[0] for p in pairs]\n",
    "target_texts = [\"<start> \" + p[1] + \" <end>\" for p in pairs]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters=\"\")\n",
    "tokenizer.fit_on_texts(input_texts + target_texts)\n",
    "\n",
    "input_seqs = tokenizer.texts_to_sequences(input_texts)\n",
    "target_seqs = tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "encoder_input = pad_sequences(input_seqs, maxlen=MAX_LEN, padding=\"post\")\n",
    "decoder_input = pad_sequences(target_seqs, maxlen=MAX_LEN, padding=\"post\")\n",
    "\n",
    "enc_train, enc_test, dec_train, dec_test = train_test_split(\n",
    "    encoder_input, decoder_input, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… Tokenization done: {enc_train.shape[0]} train samples, {enc_test.shape[0]} test samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T14:20:36.570680Z",
     "iopub.status.busy": "2025-11-05T14:20:36.570146Z",
     "iopub.status.idle": "2025-11-05T14:20:36.586486Z",
     "shell.execute_reply": "2025-11-05T14:20:36.585674Z",
     "shell.execute_reply.started": "2025-11-05T14:20:36.570660Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input shape: (250635, 20)\n",
      "Decoder input shape: (250635, 20)\n",
      "Vocabulary size: 71407\n",
      "\n",
      "ğŸ”¹ Example:\n",
      "Input: i wanted an ipad, sure, but it was more . . . it was more that i'd told myself i was winning that ipad.\n",
      "Target: <start> if i couldn't do this, something that's relatively simple, how on earth am i supposed to keep my bigger commitments? <end>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Encoder input shape:\", encoder_input.shape)\n",
    "print(\"Decoder input shape:\", decoder_input.shape)\n",
    "print(\"Vocabulary size:\", len(tokenizer.word_index))\n",
    "\n",
    "# Quick reverse check\n",
    "reverse_vocab = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# View a random training example\n",
    "idx = np.random.randint(0, len(input_texts))\n",
    "print(\"\\nğŸ”¹ Example:\")\n",
    "print(\"Input:\", input_texts[idx])\n",
    "print(\"Target:\", target_texts[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T15:11:15.344183Z",
     "iopub.status.busy": "2025-11-05T15:11:15.343873Z",
     "iopub.status.idle": "2025-11-05T15:17:54.491058Z",
     "shell.execute_reply": "2025-11-05T15:17:54.490191Z",
     "shell.execute_reply.started": "2025-11-05T15:11:15.344163Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using 30000 samples, vocab size 15000, maxlen 30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ encoder_input       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ decoder_input       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_18        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,000</span> â”‚ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_19        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,000</span> â”‚ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> â”‚ embedding_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚            â”‚                   â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚            â”‚                   â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) â”‚ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> â”‚ embedding_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚            â”‚ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚            â”‚ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ additive_attention  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AdditiveAttention</span>) â”‚                   â”‚            â”‚ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate_12      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">â€¦</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ additive_attentiâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ decoder_dense       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">7,695,000</span> â”‚ concatenate_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)            â”‚            â”‚                   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ encoder_input       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ decoder_input       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_18        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) â”‚  \u001b[38;5;34m1,920,000\u001b[0m â”‚ encoder_input[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ embedding_19        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) â”‚  \u001b[38;5;34m1,920,000\u001b[0m â”‚ decoder_input[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     â”‚    \u001b[38;5;34m394,240\u001b[0m â”‚ embedding_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      â”‚            â”‚                   â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      â”‚            â”‚                   â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m256\u001b[0m)]             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) â”‚ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     â”‚    \u001b[38;5;34m394,240\u001b[0m â”‚ embedding_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      â”‚            â”‚ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      â”‚            â”‚ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m256\u001b[0m)]             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ additive_attention  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) â”‚        \u001b[38;5;34m256\u001b[0m â”‚ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mAdditiveAttention\u001b[0m) â”‚                   â”‚            â”‚ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate_12      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34mâ€¦\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ additive_attentiâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ decoder_dense       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      â”‚  \u001b[38;5;34m7,695,000\u001b[0m â”‚ concatenate_12[\u001b[38;5;34m0\u001b[0mâ€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mDense\u001b[0m)             â”‚ \u001b[38;5;34m15000\u001b[0m)            â”‚            â”‚                   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,323,736</span> (47.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m12,323,736\u001b[0m (47.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,323,736</span> (47.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,323,736\u001b[0m (47.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "enc_train_sub: (30000, 20)\n",
      "decoder_input_trimmed: (30000, 19)\n",
      "decoder_target: (30000, 19)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step\n",
      "Epoch 1/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 84ms/step - accuracy: 0.5444 - loss: 3.7541 - val_accuracy: 0.5733 - val_loss: 2.7539 - learning_rate: 0.0010\n",
      "Epoch 2/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.5859 - loss: 2.6529 - val_accuracy: 0.5969 - val_loss: 2.5485 - learning_rate: 0.0010\n",
      "Epoch 3/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.6100 - loss: 2.4195 - val_accuracy: 0.6021 - val_loss: 2.4461 - learning_rate: 0.0010\n",
      "Epoch 4/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.6145 - loss: 2.3124 - val_accuracy: 0.6057 - val_loss: 2.3991 - learning_rate: 0.0010\n",
      "Epoch 5/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.6192 - loss: 2.2290 - val_accuracy: 0.6085 - val_loss: 2.3667 - learning_rate: 0.0010\n",
      "Epoch 6/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.6227 - loss: 2.1663 - val_accuracy: 0.6100 - val_loss: 2.3521 - learning_rate: 0.0010\n",
      "Epoch 7/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.6255 - loss: 2.1024 - val_accuracy: 0.6115 - val_loss: 2.3445 - learning_rate: 0.0010\n",
      "Epoch 8/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.6333 - loss: 2.0215 - val_accuracy: 0.6124 - val_loss: 2.3433 - learning_rate: 0.0010\n",
      "Epoch 9/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.6318 - loss: 1.9874 - val_accuracy: 0.6123 - val_loss: 2.3549 - learning_rate: 0.0010\n",
      "Epoch 10/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6372 - loss: 1.9206\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.6372 - loss: 1.9206 - val_accuracy: 0.6120 - val_loss: 2.3649 - learning_rate: 0.0010\n",
      "Epoch 11/25\n",
      "\u001b[1m422/422\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 85ms/step - accuracy: 0.6444 - loss: 1.8313 - val_accuracy: 0.6117 - val_loss: 2.3788 - learning_rate: 5.0000e-04\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "âœ… Optimized training finished and model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, AdditiveAttention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np, pickle\n",
    "\n",
    "# --- Config ---\n",
    "SAMPLE_SIZE = 30000\n",
    "MAX_LEN = 30\n",
    "VOCAB_LIMIT = 15000\n",
    "EMBED_DIM = 128\n",
    "LATENT_DIM = 256\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "\n",
    "# --- Helper: clip tokens to vocab limit ---\n",
    "def clip_tokens(sequences, vocab_size):\n",
    "    return [[min(token, vocab_size - 1) for token in seq] for seq in sequences]\n",
    "\n",
    "# --- Prepare subset ---\n",
    "enc_train_sub = np.array(clip_tokens(enc_train[:SAMPLE_SIZE].tolist(), VOCAB_LIMIT), dtype=np.int32)\n",
    "dec_train_sub = np.array(clip_tokens(dec_train[:SAMPLE_SIZE].tolist(), VOCAB_LIMIT), dtype=np.int32)\n",
    "VOCAB_SIZE = VOCAB_LIMIT\n",
    "\n",
    "print(f\"âœ… Using {SAMPLE_SIZE} samples, vocab size {VOCAB_SIZE}, maxlen {MAX_LEN}\")\n",
    "\n",
    "# --- Encoder ---\n",
    "encoder_inputs = Input(shape=(None,), name=\"encoder_input\")\n",
    "enc_emb = Embedding(VOCAB_SIZE, EMBED_DIM)(encoder_inputs)\n",
    "encoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name=\"encoder_lstm\")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "# --- Decoder ---\n",
    "decoder_inputs = Input(shape=(None,), name=\"decoder_input\")\n",
    "dec_emb = Embedding(VOCAB_SIZE, EMBED_DIM)(decoder_inputs)\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# --- Additive Attention ---\n",
    "attention = AdditiveAttention(name=\"additive_attention\")\n",
    "attn_out = attention([decoder_outputs, encoder_outputs])\n",
    "decoder_combined = Concatenate(axis=-1)([decoder_outputs, attn_out])\n",
    "\n",
    "# --- Dense projection ---\n",
    "decoder_dense = Dense(VOCAB_SIZE, activation=\"softmax\", name=\"decoder_dense\")\n",
    "decoder_outputs = decoder_dense(decoder_combined)\n",
    "\n",
    "# --- Model ---\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=LR),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# --- Shift decoder sequences for teacher forcing ---\n",
    "decoder_target = dec_train_sub[:, 1:]\n",
    "decoder_input_trimmed = dec_train_sub[:, :-1]\n",
    "decoder_target_exp = decoder_target[..., None]\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"enc_train_sub:\", enc_train_sub.shape)\n",
    "print(\"decoder_input_trimmed:\", decoder_input_trimmed.shape)\n",
    "print(\"decoder_target:\", decoder_target.shape)\n",
    "\n",
    "# --- Callbacks: learning-rate scheduler + early stopping ---\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=2, verbose=1, min_lr=1e-5\n",
    ")\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "# --- Sanity check small forward pass ---\n",
    "_ = model.predict([enc_train_sub[:2], decoder_input_trimmed[:2]])\n",
    "\n",
    "# --- Train ---\n",
    "history = model.fit(\n",
    "    [enc_train_sub, decoder_input_trimmed],\n",
    "    decoder_target_exp,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[lr_scheduler, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Save ---\n",
    "model.save(\"/kaggle/working/chatbot_seq2seq_optimized.h5\")\n",
    "with open(\"/kaggle/working/tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"âœ… Optimized training finished and model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Try Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T15:17:54.492868Z",
     "iopub.status.busy": "2025-11-05T15:17:54.492628Z",
     "iopub.status.idle": "2025-11-05T15:18:00.935388Z",
     "shell.execute_reply": "2025-11-05T15:18:00.934802Z",
     "shell.execute_reply.started": "2025-11-05T15:17:54.492849Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded with 9 layers\n",
      "Available layers: ['encoder_input', 'decoder_input', 'embedding_18', 'embedding_19', 'encoder_lstm', 'decoder_lstm', 'additive_attention', 'concatenate_12', 'decoder_dense']\n",
      "âœ… Encoder and Decoder inference models rebuilt successfully.\n",
      "\n",
      "ğŸ—£ï¸ Chatbot responses:\n",
      "\n",
      "User: hi how are you\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Bot:  i am doing well!\n",
      "\n",
      "User: what are you doing today\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Bot:  i don't know how to do it\n",
      "\n",
      "User: do you like photography\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Bot:  i don't know what about you?\n",
      "\n",
      "User: tell me about sketch app\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Bot:  i think i would be a advertising\n",
      "\n",
      "User: that's terrible\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Bot:  i think i would be able to go to the advertising\n",
      "\n",
      "User: what's your favorite color\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Bot:  i think i would be able to go to the advertising\n",
      "\n",
      "User: how's the weather\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Bot:  i think i was doing the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Combined Inference, Decoding, and Evaluation Cell ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np, pickle\n",
    "\n",
    "# --- Load model and tokenizer ---\n",
    "model = load_model(\"/kaggle/working/chatbot_seq2seq_optimized.h5\", compile=False)\n",
    "with open(\"/kaggle/working/tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "LATENT_DIM = 256\n",
    "MAX_LEN = 30\n",
    "\n",
    "print(f\"âœ… Model loaded with {len(model.layers)} layers\")\n",
    "print(\"Available layers:\", [layer.name for layer in model.layers])\n",
    "\n",
    "# --- Identify embedding layers dynamically ---\n",
    "embedding_layers = [l for l in model.layers if \"embedding\" in l.name]\n",
    "encoder_emb_layer = embedding_layers[0]\n",
    "decoder_emb_layer = embedding_layers[1]\n",
    "\n",
    "# --- Encoder inference model ---\n",
    "encoder_inputs = model.input[0]  # first input (from training)\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.get_layer(\"encoder_lstm\").output\n",
    "encoder_model = tf.keras.Model(encoder_inputs, [encoder_outputs, state_h_enc, state_c_enc])\n",
    "\n",
    "# --- Decoder inference model ---\n",
    "decoder_inputs = model.input[1]  # second input (from training)\n",
    "decoder_lstm = model.get_layer(\"decoder_lstm\")\n",
    "attn_layer = model.get_layer(\"additive_attention\")\n",
    "concat_layer = [l for l in model.layers if \"concatenate\" in l.name][0]  # auto-detect concatenate layer\n",
    "dense_layer = model.get_layer(\"decoder_dense\")\n",
    "\n",
    "# Define new inputs for decoder states and encoder outputs\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "encoder_out_input = Input(shape=(None, LATENT_DIM))\n",
    "\n",
    "# Rebuild the decoder computation graph for inference\n",
    "dec_emb_infer = decoder_emb_layer(decoder_inputs)\n",
    "dec_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    dec_emb_infer, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "attn_out_inf = attn_layer([dec_outputs, encoder_out_input])\n",
    "dec_combined_inf = concat_layer([dec_outputs, attn_out_inf])\n",
    "dec_preds = dense_layer(dec_combined_inf)\n",
    "\n",
    "decoder_model = tf.keras.Model(\n",
    "    [decoder_inputs, encoder_out_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [dec_preds, state_h_dec, state_c_dec]\n",
    ")\n",
    "\n",
    "print(\"âœ… Encoder and Decoder inference models rebuilt successfully.\")\n",
    "\n",
    "# --- Token index helpers ---\n",
    "reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "START_TOKEN = tokenizer.word_index.get(\"<start>\", 1)\n",
    "END_TOKEN = tokenizer.word_index.get(\"<end>\", 2)\n",
    "\n",
    "# --- Decoding function ---\n",
    "def decode_sequence(input_text):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text.lower()])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "    enc_outs, state_h, state_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = START_TOKEN\n",
    "\n",
    "    decoded_words = []\n",
    "    for _ in range(MAX_LEN):\n",
    "        preds, h, c = decoder_model.predict([target_seq, enc_outs, state_h, state_c])\n",
    "        next_token = np.argmax(preds[0, -1, :])\n",
    "        if next_token == END_TOKEN:\n",
    "            break\n",
    "        word = reverse_word_index.get(next_token, '')\n",
    "        if word:\n",
    "            decoded_words.append(word)\n",
    "        target_seq[0, 0] = next_token\n",
    "        state_h, state_c = h, c\n",
    "\n",
    "    return ' '.join(decoded_words)\n",
    "\n",
    "# --- Evaluate chatbot on sample inputs ---\n",
    "test_inputs = [\n",
    "    \"hi how are you\",\n",
    "    \"what are you doing today\",\n",
    "    \"do you like photography\",\n",
    "    \"tell me about sketch app\",\n",
    "    \"that's terrible\",\n",
    "    \"what's your favorite color\",\n",
    "    \"how's the weather\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ—£ï¸ Chatbot responses:\\n\")\n",
    "for inp in test_inputs:\n",
    "    print(f\"User: {inp}\")\n",
    "    print(f\"Bot:  {decode_sequence(inp)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation with BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T15:18:00.936474Z",
     "iopub.status.busy": "2025-11-05T15:18:00.936191Z",
     "iopub.status.idle": "2025-11-05T15:22:07.103323Z",
     "shell.execute_reply": "2025-11-05T15:22:07.102341Z",
     "shell.execute_reply.started": "2025-11-05T15:18:00.936451Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Evaluating quietly on 250 random samples...\n",
      "\n",
      "âœ… Average BLEU score over 250 samples: 0.0129\n",
      "\n",
      "ğŸ§© Sample qualitative outputs:\n",
      "\n",
      "Example 1\n",
      "Input:     if unicorns were real the demand for would go down clearly\n",
      "Reference: <start> that's really cool! i didn't know about that <end>\n",
      "Predicted: i think i would be able to go to the advertising\n",
      "BLEU:      0.022\n",
      "\n",
      "Example 2\n",
      "Input:     or thats how many his count went up in the last few minutes\n",
      "Reference: <start> because i have been chatting for over five hours already <end>\n",
      "Predicted: i think i would be a advertising\n",
      "BLEU:      0.016\n",
      "\n",
      "Example 3\n",
      "Input:     not at people, though. don't get any kind of bad ideas about me with hand guns\n",
      "Reference: <start> shes like and she is the first climber ever to climb a grade <end>\n",
      "Predicted: i think i was like a advertising\n",
      "BLEU:      0.012\n",
      "\n",
      "Example 4\n",
      "Input:     so i just use it for everything\n",
      "Reference: <start> yep <end>\n",
      "Predicted: i am doing well!\n",
      "BLEU:      0.000\n",
      "\n",
      "Example 5\n",
      "Input:     like isn't that the opposite of what you're supposed to do haha\n",
      "Reference: <start> haha <end>\n",
      "Predicted: i think i was really good at the\n",
      "BLEU:      0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Fully Silent Final Evaluation: BLEU & Sample Outputs ---\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# --- Silence TF logs completely ---\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "# --- Overwrite decode_sequence() to suppress progress ---\n",
    "def decode_sequence(input_text):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text.lower()])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "    enc_outs, state_h, state_c = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = START_TOKEN\n",
    "\n",
    "    decoded_words = []\n",
    "    for _ in range(MAX_LEN):\n",
    "        preds, h, c = decoder_model.predict([target_seq, enc_outs, state_h, state_c], verbose=0)\n",
    "        next_token = np.argmax(preds[0, -1, :])\n",
    "        if next_token == END_TOKEN:\n",
    "            break\n",
    "        word = reverse_word_index.get(next_token, '')\n",
    "        if word:\n",
    "            decoded_words.append(word)\n",
    "        target_seq[0, 0] = next_token\n",
    "        state_h, state_c = h, c\n",
    "\n",
    "    return ' '.join(decoded_words)\n",
    "\n",
    "# --- Evaluation ---\n",
    "num_eval = 250\n",
    "refs, hyps, bleu_scores = [], [], []\n",
    "\n",
    "print(f\"ğŸ”¹ Evaluating quietly on {num_eval} random samples...\")\n",
    "\n",
    "for i in np.random.choice(len(enc_test), num_eval, replace=False):\n",
    "    inp = ' '.join([reverse_word_index.get(idx, '') for idx in enc_test[i] if idx > 0])\n",
    "    ref = ' '.join([reverse_word_index.get(idx, '') for idx in dec_test[i] if idx > 0])\n",
    "    hyp = decode_sequence(inp)\n",
    "    refs.append(ref.split())\n",
    "    hyps.append(hyp.split())\n",
    "    bleu = sentence_bleu([ref.split()], hyp.split(), smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "# --- Results ---\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "print(f\"\\nâœ… Average BLEU score over {num_eval} samples: {avg_bleu:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ§© Sample qualitative outputs:\\n\")\n",
    "for j in range(5):\n",
    "    print(f\"Example {j+1}\")\n",
    "    print(f\"Input:     {' '.join([reverse_word_index.get(idx, '') for idx in enc_test[j] if idx > 0])}\")\n",
    "    print(f\"Reference: {' '.join(refs[j])}\")\n",
    "    print(f\"Predicted: {' '.join(hyps[j])}\")\n",
    "    print(f\"BLEU:      {bleu_scores[j]:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
