{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ================================\n# EX4: Word2Vec Skip-Gram (SGNS) — Setup & Preprocessing\n# ================================\nimport os, io, math, random, zipfile, urllib.request, numpy as np\nfrom collections import Counter\n\nrandom.seed(42)\nnp.random.seed(42)\n\n# ---- Assignment-aligned hyperparams (patched)\nEMBED_DIM     = 300          # 300-d embeddings\nWINDOW_MAX    = 5            # dynamic window in [1, 5]\nNEGATIVE_K    = 15           # stronger negative sampling (5–20 recommended)\nSUBSAMPLE_T   = 1e-5         # subsampling threshold\nINIT_LR       = 0.020        # slightly cooler start; we’ll decay token-wise\nEPOCHS        = 3            # more than 1 epoch for stability/quality\nVOCAB_MIN_CNT = 5            # prune ultra-rare tokens\nCORPUS_TOKENS = None         # use full text8 (~17M tokens)\nTABLE_SIZE    = int(2e6)     # larger negative sampler table for smoother draws\n\n# ---- Download text8 (~31MB zipped)\nurl = \"http://mattmahoney.net/dc/text8.zip\"\nlocal_zip = \"text8.zip\"\nif not os.path.exists(local_zip):\n    print(\"Downloading text8.zip...\")\n    urllib.request.urlretrieve(url, local_zip)\n    print(\"Downloaded.\")\n\n# Read tokens\nwith zipfile.ZipFile(local_zip) as zf:\n    with zf.open(\"text8\") as f:\n        text_bytes = f.read()\ntokens = io.BytesIO(text_bytes).read().decode(\"utf-8\").split()\n\nif CORPUS_TOKENS is not None:\n    tokens = tokens[:CORPUS_TOKENS]\nprint(f\"Total tokens used: {len(tokens):,}\")\n\n# ---- Build vocabulary and corpus ids\nfreq = Counter(tokens)\nvocab = {w: c for w, c in freq.items() if c >= VOCAB_MIN_CNT}\nvocab_list = sorted(vocab.keys())\nword2id = {w: i for i, w in enumerate(vocab_list)}\nid2word = np.array(vocab_list)\nV = len(word2id)\nprint(f\"Vocab size (min_count={VOCAB_MIN_CNT}): {V:,}\")\n\ncorpus = np.array([word2id[w] for w in tokens if w in word2id], dtype=np.int32)\ncounts = np.bincount(corpus, minlength=V)\ntotal_count = counts.sum()\nfreqs = counts / total_count\n\n# ---- Subsampling (patched to canonical word2vec form)\n# p_keep(w) = (sqrt(f/t) + 1) * (t/f)\nt = SUBSAMPLE_T\nf = freqs + 1e-12\np_keep = (np.sqrt(f / t) + 1.0) * (t / f)\np_keep = np.clip(p_keep, 0.0, 1.0)\n\nrand = np.random.rand(corpus.shape[0])\nkept_mask = rand < p_keep[corpus]\ncorpus = corpus[kept_mask]\nprint(f\"After subsampling: {len(corpus):,} tokens kept\")\n\n# ---- Negative sampling distribution (unigram^0.75)\nNEG_POWER = 0.75\nneg_dist = np.power(counts, NEG_POWER)\nneg_dist = neg_dist / neg_dist.sum()\n\n# Build negative table via inverse CDF sampling\ncum_probs = np.cumsum(neg_dist)\nu = np.random.rand(TABLE_SIZE)\nneg_table = np.searchsorted(cum_probs, u).astype(np.int32)\n\nprint(\"Prep complete.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-01T02:15:19.657474Z","iopub.execute_input":"2025-09-01T02:15:19.657730Z","iopub.status.idle":"2025-09-01T02:15:29.275804Z","shell.execute_reply.started":"2025-09-01T02:15:19.657707Z","shell.execute_reply":"2025-09-01T02:15:29.275038Z"}},"outputs":[{"name":"stdout","text":"Downloading text8.zip...\nDownloaded.\nTotal tokens used: 17,005,207\nVocab size (min_count=5): 71,290\nAfter subsampling: 5,567,497 tokens kept\nPrep complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ================================\n# EX4: SGNS Training (NumPy)\n# ================================\nimport time\n\nD = EMBED_DIM\n\n# Input (target) and Output (context) embeddings\nW_in  = (np.random.rand(V, D) - 0.5) / D\nW_out = (np.random.rand(V, D) - 0.5) / D\n\ndef sigmoid(x):\n    x = np.clip(x, -8, 8)       # stability\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef sample_negatives(K):\n    idx = np.random.randint(0, len(neg_table), size=K)\n    return neg_table[idx]\n\ndef train_one_epoch(corpus_ids, lr0, window_max=5, negatives=5, report_every=500_000):\n    n = len(corpus_ids)\n    t0 = time.time()\n    pairs = 0\n\n    for i, center in enumerate(corpus_ids):\n        # token-wise linear learning-rate decay across the epoch\n        progress = i / max(1, n)\n        lr = lr0 * (1.0 - progress)\n        lr = max(lr, lr0 * 0.0001)  # tiny floor to avoid freezing\n\n        # dynamic window\n        win = np.random.randint(1, window_max + 1)\n        start = max(0, i - win)\n        end   = min(n, i + win + 1)\n\n        for j in range(start, end):\n            if j == i:\n                continue\n            context = corpus_ids[j]\n\n            v_w = W_in[center]     # (D,)\n            v_c = W_out[context]   # (D,)\n\n            # ----- positive term: log σ(vw·vc)\n            score_pos = np.dot(v_w, v_c)\n            grad_pos = (1.0 - sigmoid(score_pos))  # dL/d(score)\n\n            # updates\n            W_in[center]  += lr * grad_pos * v_c\n            W_out[context]+= lr * grad_pos * v_w\n\n            # ----- negative terms: sum log σ(-vw·vn)\n            neg_ids = sample_negatives(negatives)\n            v_ns = W_out[neg_ids]                  # (K, D)\n            scores_neg = -np.dot(v_ns, v_w)        # -vw·vn\n            grad_negs = (1.0 - sigmoid(scores_neg))  # (K,)\n\n            # updates\n            W_in[center]  -= lr * np.dot(grad_negs, v_ns)          # -lr * Σ grad_k * v_nk\n            W_out[neg_ids] -= (lr * grad_negs[:, None]) * v_w[None, :]\n\n            # light clipping for stability (optional but helpful)\n            np.clip(W_in[center],   -0.5, 0.5, out=W_in[center])\n            np.clip(W_out[context], -0.5, 0.5, out=W_out[context])\n            np.clip(W_out[neg_ids], -0.5, 0.5, out=W_out[neg_ids])\n\n            pairs += 1\n\n        if (i + 1) % report_every == 0:\n            tok_per_s = (i + 1) / (time.time() - t0 + 1e-9)\n            print(f\"[progress] seen {i+1:,}/{n:,} tokens | ~{tok_per_s:,.0f} tok/s\")\n\n    elapsed = time.time() - t0\n    print(f\"Epoch done: processed {pairs:,} (center,context) pairs in {elapsed:.1f}s\")\n\n# ---- Train for multiple epochs with shuffle each pass\nfor ep in range(EPOCHS):\n    order = np.random.permutation(len(corpus))\n    corpus_shuf = corpus[order]\n    lr0 = INIT_LR * (1 - ep / max(1, EPOCHS))  # epoch-level mild decay\n    print(f\"\\n=== Epoch {ep+1}/{EPOCHS} | base_lr={lr0:.5f} ===\")\n    train_one_epoch(corpus_shuf, lr0, window_max=WINDOW_MAX, negatives=NEGATIVE_K, report_every=500_000)\n\nprint(\"\\nTraining finished. Embeddings in W_in (target) / W_out (context).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T02:16:05.063440Z","iopub.execute_input":"2025-09-01T02:16:05.064065Z","iopub.status.idle":"2025-09-01T05:39:41.415966Z","shell.execute_reply.started":"2025-09-01T02:16:05.064039Z","shell.execute_reply":"2025-09-01T05:39:41.415286Z"}},"outputs":[{"name":"stdout","text":"\n=== Epoch 1/3 | base_lr=0.02000 ===\n[progress] seen 500,000/5,567,497 tokens | ~1,386 tok/s\n[progress] seen 1,000,000/5,567,497 tokens | ~1,400 tok/s\n[progress] seen 1,500,000/5,567,497 tokens | ~1,415 tok/s\n[progress] seen 2,000,000/5,567,497 tokens | ~1,422 tok/s\n[progress] seen 2,500,000/5,567,497 tokens | ~1,425 tok/s\n[progress] seen 3,000,000/5,567,497 tokens | ~1,429 tok/s\n[progress] seen 3,500,000/5,567,497 tokens | ~1,435 tok/s\n[progress] seen 4,000,000/5,567,497 tokens | ~1,433 tok/s\n[progress] seen 4,500,000/5,567,497 tokens | ~1,430 tok/s\n[progress] seen 5,000,000/5,567,497 tokens | ~1,424 tok/s\n[progress] seen 5,500,000/5,567,497 tokens | ~1,424 tok/s\nEpoch done: processed 33,409,326 (center,context) pairs in 3912.1s\n\n=== Epoch 2/3 | base_lr=0.01333 ===\n[progress] seen 500,000/5,567,497 tokens | ~1,397 tok/s\n[progress] seen 1,000,000/5,567,497 tokens | ~1,429 tok/s\n[progress] seen 1,500,000/5,567,497 tokens | ~1,446 tok/s\n[progress] seen 2,000,000/5,567,497 tokens | ~1,422 tok/s\n[progress] seen 2,500,000/5,567,497 tokens | ~1,427 tok/s\n[progress] seen 3,000,000/5,567,497 tokens | ~1,428 tok/s\n[progress] seen 3,500,000/5,567,497 tokens | ~1,417 tok/s\n[progress] seen 4,000,000/5,567,497 tokens | ~1,408 tok/s\n[progress] seen 4,500,000/5,567,497 tokens | ~1,397 tok/s\n[progress] seen 5,000,000/5,567,497 tokens | ~1,389 tok/s\n[progress] seen 5,500,000/5,567,497 tokens | ~1,383 tok/s\nEpoch done: processed 33,399,245 (center,context) pairs in 4028.6s\n\n=== Epoch 3/3 | base_lr=0.00667 ===\n[progress] seen 500,000/5,567,497 tokens | ~1,312 tok/s\n[progress] seen 1,000,000/5,567,497 tokens | ~1,314 tok/s\n[progress] seen 1,500,000/5,567,497 tokens | ~1,325 tok/s\n[progress] seen 2,000,000/5,567,497 tokens | ~1,323 tok/s\n[progress] seen 2,500,000/5,567,497 tokens | ~1,317 tok/s\n[progress] seen 3,000,000/5,567,497 tokens | ~1,308 tok/s\n[progress] seen 3,500,000/5,567,497 tokens | ~1,301 tok/s\n[progress] seen 4,000,000/5,567,497 tokens | ~1,300 tok/s\n[progress] seen 4,500,000/5,567,497 tokens | ~1,299 tok/s\n[progress] seen 5,000,000/5,567,497 tokens | ~1,300 tok/s\n[progress] seen 5,500,000/5,567,497 tokens | ~1,302 tok/s\nEpoch done: processed 33,414,549 (center,context) pairs in 4274.3s\n\nTraining finished. Embeddings in W_in (target) / W_out (context).\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ================================\n# EX4: Embedding utilities & quick evaluation\n# ================================\nfrom numpy.linalg import norm\n\n# Use input embeddings as word vectors\nE = W_in / (norm(W_in, axis=1, keepdims=True) + 1e-9)\n\ndef most_similar(word, topn=10):\n    if word not in word2id:\n        return []\n    wid = word2id[word]\n    v = E[wid]\n    sims = E @ v\n    sims[wid] = -1.0  # exclude self\n    top = sims.argsort()[-topn:][::-1]\n    return [(id2word[i], float(sims[i])) for i in top]\n\ndef analogy(a, b, c, topn=10):\n    for w in (a, b, c):\n        if w not in word2id:\n            return []\n    va, vb, vc = E[word2id[a]], E[word2id[b]], E[word2id[c]]\n    query = vb - va + vc\n    query = query / (norm(query) + 1e-9)\n    sims = E @ query\n    for w in (a, b, c):\n        sims[word2id[w]] = -1.0\n    top = sims.argsort()[-topn:][::-1]\n    return [(id2word[i], float(sims[i])) for i in top]\n\ndef show_neighbors(words, topn=10):\n    for w in words:\n        if w not in word2id:\n            print(f\"\\n'{w}' not in vocab.\")\n            continue\n        res = most_similar(w, topn=topn)\n        print(f\"\\nNearest to '{w}':\")\n        for t, sc in res:\n            print(f\"  {t:>12s}  {sc: .4f}\")  # 4 decimals to see spread\n\nprobe_words = [\"king\", \"queen\", \"man\", \"woman\", \"city\", \"music\", \"science\"]\nshow_neighbors(probe_words, topn=10)\n\ntests = [(\"man\",\"king\",\"woman\"), (\"paris\",\"france\",\"rome\"), (\"good\",\"better\",\"bad\")]\nfor a, b, c in tests:\n    res = analogy(a, b, c, topn=5)\n    if res:\n        print(f\"\\nAnalogy: {a}:{b} :: {c}:?\")\n        for t, sc in res[:5]:\n            print(f\"  {t:>12s}  {sc: .4f}\")\n\n# Save vectors (for later use)\nnp.savez_compressed(\"sgns_text8_embeddings.npz\", W_in=W_in, W_out=W_out, id2word=id2word)\nprint(\"\\nSaved: sgns_text8_embeddings.npz\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-01T05:45:29.601380Z","iopub.execute_input":"2025-09-01T05:45:29.601894Z","iopub.status.idle":"2025-09-01T05:45:46.439583Z","shell.execute_reply.started":"2025-09-01T05:45:29.601871Z","shell.execute_reply":"2025-09-01T05:45:46.438751Z"}},"outputs":[{"name":"stdout","text":"\nNearest to 'king':\n         whole   0.9957\n        during   0.9948\n       dragons   0.9945\n  supernatural   0.9942\n       voltage   0.9939\n       friends   0.9939\n         those   0.9939\n      republic   0.9938\n        ethnic   0.9938\n           bus   0.9938\n\nNearest to 'queen':\n        ground   0.9972\n       supreme   0.9970\n     executive   0.9968\n       shorter   0.9968\n      criteria   0.9964\n         teeth   0.9964\n         japan   0.9963\n        enough   0.9963\n          last   0.9963\n        formal   0.9962\n\nNearest to 'man':\n        states   0.9938\n        plague   0.9938\n      criteria   0.9936\n         solve   0.9935\n        ethnic   0.9935\n          been   0.9934\n     australia   0.9933\n        joined   0.9933\n     compounds   0.9932\n          land   0.9931\n\nNearest to 'woman':\n   appointment   0.9981\n     obviously   0.9980\n     manifesto   0.9978\n           try   0.9978\n     volunteer   0.9977\n     microwave   0.9977\n     auxiliary   0.9976\n       rounded   0.9976\n       derives   0.9976\n       implied   0.9976\n\nNearest to 'city':\n           cia   0.9954\n      teachers   0.9953\n         pages   0.9952\n          uses   0.9951\n       roughly   0.9951\n        league   0.9951\n        allied   0.9950\n        sought   0.9950\n    connection   0.9947\n         input   0.9947\n\nNearest to 'music':\n     arguments   0.9967\n      ancestor   0.9966\n       charity   0.9962\n        galaxy   0.9962\n          cost   0.9961\n        lawyer   0.9961\n     rebellion   0.9961\n      superior   0.9960\n          trek   0.9960\n    comprising   0.9960\n\nNearest to 'science':\n          iran   0.9951\n           too   0.9950\n   influential   0.9948\n     communist   0.9947\n    california   0.9946\n          took   0.9945\n         decay   0.9944\n         fruit   0.9943\n      variable   0.9942\n  constitution   0.9941\n\nAnalogy: man:king :: woman:?\n     questions   0.9882\n        victor   0.9875\n         kevin   0.9872\n     roosevelt   0.9871\n   appointment   0.9870\n\nAnalogy: paris:france :: rome:?\n        infant   0.9941\n          arts   0.9938\n         birth   0.9938\n         youth   0.9935\n       october   0.9935\n\nAnalogy: good:better :: bad:?\n          solo   0.9905\n     reactions   0.9905\n        freely   0.9903\n         cross   0.9900\n          audi   0.9899\n\nSaved: sgns_text8_embeddings.npz\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}